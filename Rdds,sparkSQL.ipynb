{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP4Z5Uz3yjSV"
      },
      "outputs": [],
      "source": [
        "# 1. Install Java\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# 2. Download Spark 3.5.0 with Hadoop 3\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "\n",
        "# 3. Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Install findspark\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "eg8mkQh9yxgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Start Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"RDD Example\").getOrCreate()"
      ],
      "metadata": {
        "id": "u4SbtvGUy8Kg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "Daryba5EzEcd",
        "outputId": "69b2680a-659b-43d8-a631-3e4046787359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7e22fcb747d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://d30dc5d17aa7:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>RDD Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a dataframe\n",
        "df = spark.read.csv(\"/content/sample_data/sample_census_data.csv\",header=True,inferSchema=True)"
      ],
      "metadata": {
        "id": "OjKc-YjvzT5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert dataframe to rdd\n",
        "census_rdd=df.rdd"
      ],
      "metadata": {
        "id": "QfXZBYlybjEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show the rdd's contents using collect()\n",
        "census_rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LomhAsuIbwch",
        "outputId": "9a00ea5a-5eff-4dae-d5bd-6a761a6d6c08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(State='California', Population=39538223, Median_Age=36.5, Households=13000000, Average_Income=78000, Year=2020),\n",
              " Row(State='Texas', Population=29145505, Median_Age=34.8, Households=9800000, Average_Income=67000, Year=2020),\n",
              " Row(State='Florida', Population=21538187, Median_Age=42.0, Households=8000000, Average_Income=61000, Year=2020),\n",
              " Row(State='New York', Population=20201249, Median_Age=39.0, Households=7500000, Average_Income=75000, Year=2020),\n",
              " Row(State='Illinois', Population=12812508, Median_Age=38.5, Households=4900000, Average_Income=68000, Year=2020)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataFrames   vs RDDs\n",
        "| Feature | RDD | DataFrame |\n",
        "|----------|-----|------------|\n",
        "| **Abstraction Level** | Low-level; requires manual operations | High-level; easy to use with SQL-like syntax |\n",
        "| **Optimization** | No automatic optimization | Optimized by Catalyst engine |\n",
        "| **Schema Information** | Does not store schema | Has schema (structured data) |\n",
        "| **Use Case** | Complex, custom transformations | Structured data analysis and SQL queries |\n"
      ],
      "metadata": {
        "id": "zMtB4fFJhTE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SPARK SQL\n",
        "Module in Apache Spark for processing structured and semi-structured data using SQL Syntax."
      ],
      "metadata": {
        "id": "1Pz6Ptgqhl5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Temporary Tables**-Register data frame as temporary view\n"
      ],
      "metadata": {
        "id": "3Y8LoP5NinIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample df\n",
        "data = [(\"Alice\",\"HR\",30),(\"Bob\",\"IT\",40),(\"Cathy\",\"HR\",27)]\n",
        "columns=[\"Name\",\"Department\",\"Age\"]\n",
        "df1=spark.createDataFrame(data,schema=columns)\n",
        "\n",
        "# Register df1 as a temporary view\n",
        "df1.createOrReplaceTempView(\"People\")"
      ],
      "metadata": {
        "id": "FplNNhDkheXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#query using SQL\n",
        "result = spark.sql(\"SELECT Name,Age FROM People WHERE Age>30\")\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKH3-yrvkRto",
        "outputId": "cefc1627-a47c-4db3-afaa-2b354ed35909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+\n",
            "|Name|Age|\n",
            "+----+---+\n",
            "| Bob| 40|\n",
            "+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**READING FILES as temporary views**"
      ],
      "metadata": {
        "id": "jRmnudqxJNFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2=spark.read.csv(\"/content/sample_data/employee_table.csv\",header=True,inferSchema=True)"
      ],
      "metadata": {
        "id": "MzSAnZTfJWJl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hc0e5zGKWxZ",
        "outputId": "2355bd38-1b79-4e30-99e4-93dac33801c0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---+------+\n",
            "|   name|department|age|salary|\n",
            "+-------+----------+---+------+\n",
            "|  Alice|        HR| 28| 48000|\n",
            "|    Bob|   Finance| 34| 56000|\n",
            "|Charlie|        IT| 30| 72000|\n",
            "|  Diana| Marketing| 27| 50000|\n",
            "|  Ethan|        IT| 40| 85000|\n",
            "|  Fiona|        HR| 29| 47000|\n",
            "+-------+----------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#register to a temporary view\n",
        "df2.createOrReplaceTempView(\"employees\")"
      ],
      "metadata": {
        "id": "97pwHYGaJ79W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Combining SQL and Dataframe Operation\n",
        "query_result=spark.sql(\"SELECT name,salary FROM employees WHERE salary >50000\")"
      ],
      "metadata": {
        "id": "B4ySF-g7kham"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataframe transformation\n",
        "high_earners=query_result.withColumn(\"bonus\",query_result.salary * 0.1)"
      ],
      "metadata": {
        "id": "y3tTgpxPKO5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_earners.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2QOozfoK9xb",
        "outputId": "48c1aa9c-e3e0-4228-fce5-a024be49c471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+------+\n",
            "|   name|salary| bonus|\n",
            "+-------+------+------+\n",
            "|    Bob| 56000|5600.0|\n",
            "|Charlie| 72000|7200.0|\n",
            "|  Ethan| 85000|8500.0|\n",
            "+-------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pyspark SQL Aggregations"
      ],
      "metadata": {
        "id": "5mF90rEbLWNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\" SELECT department,sum(salary) AS Total_Salary,Avg(salary) AS Average FROM employees Group BY department \"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m189RxFGLDaW",
        "outputId": "2789c434-439b-4972-dacc-4bfba37fc4e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-------+\n",
            "|department|Total_Salary|Average|\n",
            "+----------+------------+-------+\n",
            "|        HR|       95000|47500.0|\n",
            "|   Finance|       56000|56000.0|\n",
            "| Marketing|       50000|50000.0|\n",
            "|        IT|      157000|78500.0|\n",
            "+----------+------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Filtered views*"
      ],
      "metadata": {
        "id": "ksaFDSHFMes1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#filter salaries over 50000\n",
        "filtered_df2=df2.filter(df2.salary>50000)\n",
        "\n",
        "#register filtered dataframe as view\n",
        "filtered_df2.createOrReplaceTempView(\"filtered_employees\")"
      ],
      "metadata": {
        "id": "A8ucCwDsMRcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#aggregate using SQL\n",
        "spark.sql(\"\"\" SELECT department,count(*) AS Employee_Count FROM filtered_employees Group By department \"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2Vzve6uNDhb",
        "outputId": "e715b1eb-3935-43d2-e403-057386640a68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+\n",
            "|department|Employee_Count|\n",
            "+----------+--------------+\n",
            "|   Finance|             1|\n",
            "|        IT|             2|\n",
            "+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Type Casting"
      ],
      "metadata": {
        "id": "Iu6vVCpaOADb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = [(\"HR\",\"3000\"),(\"IT\",\"4000\"),(\"Finance\",\"3500\")]\n",
        "columns = [\"Department\", \"Salary\"]\n",
        "df3= spark.createDataFrame(data1, schema=columns)"
      ],
      "metadata": {
        "id": "KrSlnN5fNjDs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hgX_CHKOqwD",
        "outputId": "c7b010ac-1880-4a74-bbb5-88171f5835b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+\n",
            "|Department|Salary|\n",
            "+----------+------+\n",
            "|        HR|  3000|\n",
            "|        IT|  4000|\n",
            "|   Finance|  3500|\n",
            "+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert Salary column to integer\n",
        "df3 = df3.withColumn(\"Salary\", df3[\"Salary\"].cast(\"int\"))"
      ],
      "metadata": {
        "id": "g26hXeyrOtv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#perform aggregation\n",
        "df3.groupBy(\"Department\").sum(\"Salary\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLZB2tPEO_kl",
        "outputId": "6957dc1c-4484-4a49-9f1e-deb29c955772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|Department|sum(Salary)|\n",
            "+----------+-----------+\n",
            "|        HR|       3000|\n",
            "|   Finance|       3500|\n",
            "|        IT|       4000|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RDDs for aggregations**"
      ],
      "metadata": {
        "id": "ZT_twdWDPeU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=df3.rdd.map(lambda row:(row[\"Department\"],row[\"Salary\"]))"
      ],
      "metadata": {
        "id": "nH2GX4XFPkFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_agg=rdd.reduceByKey(lambda x,y:x+y)"
      ],
      "metadata": {
        "id": "5X4GJOTuPyQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rdd_agg.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0XlD3QMP7th",
        "outputId": "a3d7ca0e-e5ff-41b5-e6db-88f84a81a403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('HR', 3000), ('IT', 4000), ('Finance', 3500)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Execution Plans**"
      ],
      "metadata": {
        "id": "p6EfU-avQQI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df3.filter(df3.Salary>3500).select(\"Department\").explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyIxoXJ5QIXl",
        "outputId": "492788fa-518f-4be1-85fe-68e36aa7eee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "*(1) Project [Department#119]\n",
            "+- *(1) Filter (isnotnull(Salary#120) AND (cast(Salary#120 as int) > 3500))\n",
            "   +- *(1) Scan ExistingRDD[Department#119,Salary#120]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Caching and Persisting DataFrames**\n",
        "\n",
        "\n",
        "*   Caching: Stores data in memory for faster access of smaller datasets.\n",
        "*   Persisting:Stores data in different storage level for large datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "bUIT4YYiQ8Vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df3.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nze9K4o6RsiE",
        "outputId": "aa56180d-a14f-4481-d5bc-67892516b5dc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Department: string, Salary: string]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df3.filter(df3[\"Salary\"]>3000).show()"
      ],
      "metadata": {
        "id": "QHKE_tMRSBrU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b343276-512d-4703-b08b-0dbeb810a07b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+\n",
            "|Department|Salary|\n",
            "+----------+------+\n",
            "|        IT|  4000|\n",
            "|   Finance|  3500|\n",
            "+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df3.groupBy([\"Department\"]).count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uVVF_8uH5iv",
        "outputId": "2d329fb6-749a-430b-87fd-67de1fe2191c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|Department|count|\n",
            "+----------+-----+\n",
            "|        HR|    1|\n",
            "|   Finance|    1|\n",
            "|        IT|    1|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Persisting the DataFrame**"
      ],
      "metadata": {
        "id": "msOoe_rZIHrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import StorageLevel"
      ],
      "metadata": {
        "id": "YkK-DQ-UIC5G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#persist a dataframe to storage level\n",
        "df2.persist(StorageLevel.MEMORY_AND_DISK)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOdd3ObtIXbt",
        "outputId": "54dab7a9-10c0-419e-b915-f37053630135"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[name: string, department: string, age: int, salary: int]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#perform transformations\n",
        "result1= df2.groupBy(\"department\").agg({\"Salary\":\"sum\"})"
      ],
      "metadata": {
        "id": "8So7AfziIjiW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxCmKoZtJPea",
        "outputId": "c4230093-4e2e-40bc-cf33-9a711c0a7e9c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|department|sum(Salary)|\n",
            "+----------+-----------+\n",
            "|        HR|      95000|\n",
            "|   Finance|      56000|\n",
            "| Marketing|      50000|\n",
            "|        IT|     157000|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#unpersist after use\n",
        "df2.unpersist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMtQ5K4RJTb4",
        "outputId": "c0a631ce-afc6-41b4-aaf3-d4a852dca040"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[name: string, department: string, age: int, salary: int]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizing PySpark**\n",
        "\n",
        "\n",
        "*   Small subsections: eg-pick tools like map() over groupBy().\n",
        "*   Broadcast Joins: It uses all compute even on smaller datasets, also avoid shuffles.\n",
        "\n",
        "\n",
        "*   Avoid repeated actions: Costs time and compute if its same data.i.e Avoid using count(),show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UzyFCTaGJ4Ee"
      }
    }
  ]
}